{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T18:06:40.982356512Z",
     "start_time": "2023-11-10T18:06:40.960993415Z"
    }
   },
   "outputs": [],
   "source": [
    "import elasticsearch.helpers\n",
    "import json\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific ElasticsearchWarning\n",
    "warnings.filterwarnings(\"ignore\", category=elasticsearch.exceptions.ElasticsearchWarning)\n",
    "\n",
    "def success_at_1 (relevant, retrieved):\n",
    "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def success_at_5(relevant, retrieved):\n",
    "    for retrieved_single in retrieved[:5]:\n",
    "        if retrieved_single in relevant:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def success_at_10(relevant, retrieved):\n",
    "    for retrieved_single in retrieved[:10]:\n",
    "        if retrieved_single in relevant:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def precision(relevant, retrieved):\n",
    "    relevant_items_retrieved_counter = 0\n",
    "    for r in retrieved:\n",
    "        if r in relevant:\n",
    "            relevant_items_retrieved_counter += 1\n",
    "    return relevant_items_retrieved_counter / len(retrieved) if len(retrieved) > 0 else 0.0\n",
    "    \n",
    "def recall(relevant, retrieved):\n",
    "    relevant_items_retrieved_counter = 0\n",
    "    for r in retrieved:\n",
    "        if r in relevant:\n",
    "            relevant_items_retrieved_counter += 1\n",
    "    return relevant_items_retrieved_counter / len(relevant)\n",
    "    \n",
    "def f_measure(relevant, retrieved):\n",
    "    P = precision(relevant, retrieved)\n",
    "    R = recall(relevant, retrieved)\n",
    "    if P + R == 0.0:\n",
    "        return 0.0\n",
    "    return 2 * P * R / (P + R)\n",
    "    \n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    return precision(relevant, retrieved[:k])\n",
    "\n",
    "def r_precision(relevant, retrieved):\n",
    "    return precision(relevant, retrieved[:len(relevant)])\n",
    "\n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "    max_precision = 0.0\n",
    "    for i in range(len(retrieved)):\n",
    "        if recall(relevant, retrieved[:i]) >= X:\n",
    "            precision_at_i = precision_at_k(relevant, retrieved, i)\n",
    "            if precision_at_i > max_precision:\n",
    "                max_precision = precision_at_i\n",
    "    return max_precision\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    sum_avg = 0.0\n",
    "    for retrieved_index in range(len(retrieved)):\n",
    "        if retrieved[retrieved_index] in relevant:\n",
    "            sum_avg += precision_at_k(relevant, retrieved, retrieved_index + 1)\n",
    "    return sum_avg / len(relevant)\n",
    "    \n",
    "def read_qrels_file(qrels_file):  # reads the content of he qrels file\n",
    "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if rel == \"1\":\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    # read the content of the run file produced by our IR system \n",
    "    # (in the following exercises you will create your own run_files)\n",
    "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):    \n",
    "    sum_map = []\n",
    "    for key in all_retrieved:\n",
    "        sum_map.append(average_precision(all_relevant[key], all_retrieved[key]))\n",
    "    return sum(sum_map) / len(sum_map)\n",
    "\n",
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count\n",
    "\n",
    "def trec_eval(qrels_file, run_file):\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [success_at_1,\n",
    "               success_at_5,\n",
    "               success_at_10,\n",
    "               r_precision,\n",
    "               precision_at_1,\n",
    "               precision_at_5,\n",
    "               precision_at_10,\n",
    "               precision_at_50,\n",
    "               precision_at_100,\n",
    "               precision_at_recall_00,\n",
    "               precision_at_recall_01,\n",
    "               precision_at_recall_02,\n",
    "               precision_at_recall_03,\n",
    "               precision_at_recall_04,\n",
    "               precision_at_recall_05,\n",
    "               precision_at_recall_06,\n",
    "               precision_at_recall_07,\n",
    "               precision_at_recall_08,\n",
    "               precision_at_recall_09,\n",
    "               precision_at_recall_10,\n",
    "               average_precision,\n",
    "               f_measure,\n",
    "               recall]\n",
    "    return [mean_metric(metric, all_relevant, all_retrieved) for metric in metrics]\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results_var = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results_var:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))\n",
    "        \n",
    "def sign_test_values(measure, qrels_file, run_file_1, run_file_2):\n",
    "    all_relevant = read_qrels_file(qrels_file)\n",
    "    all_retrieved_1 = read_run_file(run_file_1)\n",
    "    all_retrieved_2 = read_run_file(run_file_2)\n",
    "    better = 0\n",
    "    worse  = 0\n",
    "    \n",
    "    for key in all_relevant:\n",
    "        measure_file_1 = measure(all_relevant[key], all_retrieved_1[key])\n",
    "        measure_file_2 = measure(all_relevant[key], all_retrieved_2[key])\n",
    "        if measure_file_1 > measure_file_2:\n",
    "            better += 1\n",
    "        elif measure_file_1 <  measure_file_2:\n",
    "            worse += 1\n",
    "    \n",
    "    return better, worse\n",
    "    \n",
    "def precision_at_rank_5(rel, ret):\n",
    "    return precision_at_k(rel, ret, k=5)\n",
    "\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if 'index' in doc_line:\n",
    "                id_param = doc_line['index']['_id']\n",
    "            elif 'PMID' in doc_line:\n",
    "                doc_line['_id'] = id_param\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es_param, index_name, body=None):\n",
    "    # delete index when it already exists\n",
    "    if body is None:\n",
    "        body = {}\n",
    "    es_param.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es_param.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es_param, collection_file_name, index_name, body=None):\n",
    "    if body is None:\n",
    "        body = {}\n",
    "    create_index(es_param, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es_param, \n",
    "        read_documents(collection_file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )\n",
    "\n",
    "def make_trec_run(es_param, topics_file_name, run_file_name, index_name=\"genomics\", run_name=\"test\"):\n",
    "    with open(f\"run/{run_file_name}\", 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                \n",
    "                search_query = {\n",
    "                    \"query\": {\n",
    "                        \"bool\": {\n",
    "                            \"should\": [\n",
    "                                {\"match\": {\"TI\": query}},  # Search in the title field.\n",
    "                                {\"match\": {\"AB\": query}}   # Search in the abstract field.\n",
    "                            ]\n",
    "                        }\n",
    "                    },\n",
    "                    \"size\": 1000  # You can adjust the number of results as needed.\n",
    "                }\n",
    "                \n",
    "                # Execute the query against the specified Elasticsearch index.\n",
    "                search_results = es_param.search(index=index_name, body=search_query)\n",
    "                \n",
    "                # Process the search results and write them to the run file in TREC format.\n",
    "                for rank, hit in enumerate(search_results['hits']['hits']):\n",
    "                    pmid = hit['_source']['PMID']\n",
    "                    score = hit['_score']\n",
    "                    run_line = f\"{qid} Q0 {pmid} {rank + 1} {score} {run_name}\\n\"\n",
    "                    run_file.write(run_line)\n",
    "                # \n",
    "\n",
    "def evaluate_models(models_param):\n",
    "    results_var = {}\n",
    "    for model_name in models_param:\n",
    "        run_file_name = f\"run/{model_name}.run\"\n",
    "        eval_results = trec_eval('data01/FIR-s05-training-qrels.txt', run_file_name)\n",
    "        results_var[model_name] = {metric: score for metric, score in eval_results}\n",
    "    return results_var\n",
    "\n",
    "def determine_best_model(results_param, metric):\n",
    "    \"\"\"Determine the best model based on a specific metric.\"\"\"\n",
    "    return max(results_param, key=lambda model: results_param[model][metric])\n",
    "\n",
    "def print_results_table(results_param):\n",
    "    # Extract model names\n",
    "    model_names = list(results_param.keys())\n",
    "    \n",
    "    # Extract metric names from the first model's results\n",
    "    metrics = list(next(iter(results_param.values())).keys())\n",
    "    \n",
    "    # Create a table with headers\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metric\"] + model_names\n",
    "    \n",
    "    # Add rows for each metric's results across all models\n",
    "    for metric in metrics:\n",
    "        values = [metric] + [f\"{results_param[model_name][metric]:.4f}\" for model_name in model_names]\n",
    "        table.add_row(values)\n",
    "    \n",
    "    # Print the table\n",
    "    print(table)\n",
    "\n",
    "    # Determine and print the best model based on mean average precision (MAP)\n",
    "    best_model = determine_best_model(results_param, 'mean average_precision')\n",
    "    print(f\"\\n\\033[1;34mThe best model based on mean average precision (MAP) is: {best_model}\\033[0m\")  # Emphasize with blue color\n",
    "    \n",
    "def generate_body(similarity_strategy, tokenizer_strategy):\n",
    "    return {\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"similarity\": {\n",
    "                    \"custom_similarity\": similarity_strategy\n",
    "                }\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"tokenizer\": {\n",
    "                    \"my_tokenizer\": tokenizer_strategy\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"my_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"my_tokenizer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"TI\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"similarity\": \"custom_similarity\",\n",
    "                    \"analyzer\": \"my_analyzer\"\n",
    "                },\n",
    "                \"AB\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"similarity\": \"custom_similarity\",\n",
    "                    \"analyzer\": \"my_analyzer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "es = elasticsearch.Elasticsearch('http://localhost:9200')\n",
    "\n",
    "def indexing(similarity_strategy, tokenizer_strategy):\n",
    "    similarity_strategy_type = similarity_strategy[\"type\"].lower()\n",
    "    tokenizer_strategy_type = tokenizer_strategy[\"type\"].lower()\n",
    "    generated_body_v = generate_body(similarity_strategy, tokenizer_strategy)\n",
    "    f_name = f\"{similarity_strategy_type}_{tokenizer_strategy_type}\"\n",
    "    index_documents(es, 'data01/FIR-s05-medline.json', f\"genomics-{f_name}\", body=generated_body_v)\n",
    "    return f_name\n",
    "\n",
    "def make_run(body_type_file):\n",
    "    make_trec_run(es, 'data01/FIR-s05-training-queries-simple.txt', f\"{body_type_file}.run\", f\"genomics-{body_type_file}\")\n",
    "    \n",
    "dfr = {\n",
    "    \"type\": \"DFR\",\n",
    "    \"basic_model\": \"g\",\n",
    "    \"after_effect\": \"l\",\n",
    "    \"normalization\": \"h2\",\n",
    "    \"normalization.h2.c\": \"3.0\"\n",
    "}\n",
    "\n",
    "bm25 =  {\n",
    "    \"type\": \"BM25\",\n",
    "    \"b\": 0.75, \n",
    "    \"k1\": 1.5\n",
    "}\n",
    "\n",
    "standard = {\n",
    "    \"type\": \"standard\"\n",
    "}\n",
    "\n",
    "whitespace = {\n",
    "    \"type\": \"whitespace\"\n",
    "}\n",
    "\n",
    "ngram = {\n",
    "    \"type\": \"ngram\",\n",
    "    \"min_gram\": 4,\n",
    "    \"max_gram\": 5,\n",
    "    \"token_chars\": [\n",
    "        \"letter\",\n",
    "        \"digit\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "edge_ngram = {\n",
    "    \"type\": \"edge_ngram\",\n",
    "    \"min_gram\": 2,\n",
    "    \"max_gram\": 20,\n",
    "    \"token_chars\": [\n",
    "        \"letter\",\n",
    "        \"digit\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(dfr, standard)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(dfr, whitespace)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(dfr, ngram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(dfr, edge_ngram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12729/313921455.py:211: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_param.indices.delete(index=index_name, ignore=[400, 404])\n",
      "/tmp/ipykernel_12729/313921455.py:220: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  return elasticsearch.helpers.bulk(\n"
     ]
    }
   ],
   "source": [
    "body_type = indexing(bm25, standard)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:12:44.866178336Z",
     "start_time": "2023-11-10T18:11:26.668056133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(bm25, whitespace)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(bm25, ngram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = indexing(bm25, edge_ngram)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "make_run(body_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:13:36.191373996Z",
     "start_time": "2023-11-10T18:13:29.150580163Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+--------+--------------+----------------+-----------+----------------+\n",
      "|            Metric           | baseline |  dfr   | dfr_standard | dfr_whitespace | dfr_ngram | dfr_edge_ngram |\n",
      "+-----------------------------+----------+--------+--------------+----------------+-----------+----------------+\n",
      "|      mean success_at_1      |  0.1053  | 0.1579 |    0.0789    |     0.0526     |   0.1053  |     0.1053     |\n",
      "|      mean success_at_5      |  0.2632  | 0.2895 |    0.2105    |     0.1053     |   0.2368  |     0.2368     |\n",
      "|      mean success_at_10     |  0.3158  | 0.4211 |    0.3158    |     0.1316     |   0.2895  |     0.2895     |\n",
      "|       mean r_precision      |  0.0916  | 0.1188 |    0.0511    |     0.0566     |   0.0730  |     0.0870     |\n",
      "|     mean precision_at_1     |  0.1053  | 0.1579 |    0.0789    |     0.0526     |   0.1053  |     0.1053     |\n",
      "|     mean precision_at_5     |  0.0789  | 0.0947 |    0.0684    |     0.0368     |   0.0632  |     0.0632     |\n",
      "|     mean precision_at_10    |  0.0474  | 0.0658 |    0.0526    |     0.0211     |   0.0421  |     0.0421     |\n",
      "|     mean precision_at_50    |  0.0195  | 0.0253 |    0.0221    |     0.0079     |   0.0163  |     0.0189     |\n",
      "|    mean precision_at_100    |  0.0139  | 0.0158 |    0.0139    |     0.0047     |   0.0111  |     0.0113     |\n",
      "| mean precision_at_recall_00 |  0.2015  | 0.2387 |    0.1518    |     0.0928     |   0.1832  |     0.1761     |\n",
      "| mean precision_at_recall_01 |  0.1898  | 0.2387 |    0.1496    |     0.0921     |   0.1783  |     0.1735     |\n",
      "| mean precision_at_recall_02 |  0.1683  | 0.2289 |    0.1456    |     0.0907     |   0.1526  |     0.1466     |\n",
      "| mean precision_at_recall_03 |  0.1333  | 0.2001 |    0.1175    |     0.0640     |   0.1498  |     0.1352     |\n",
      "| mean precision_at_recall_04 |  0.1236  | 0.1795 |    0.0882    |     0.0513     |   0.1290  |     0.1113     |\n",
      "| mean precision_at_recall_05 |  0.1227  | 0.1732 |    0.0845    |     0.0512     |   0.1170  |     0.1050     |\n",
      "| mean precision_at_recall_06 |  0.0874  | 0.1148 |    0.0523    |     0.0407     |   0.0557  |     0.0775     |\n",
      "| mean precision_at_recall_07 |  0.0843  | 0.1109 |    0.0492    |     0.0399     |   0.0537  |     0.0704     |\n",
      "| mean precision_at_recall_08 |  0.0600  | 0.0772 |    0.0415    |     0.0323     |   0.0503  |     0.0676     |\n",
      "| mean precision_at_recall_09 |  0.0580  | 0.0764 |    0.0412    |     0.0323     |   0.0499  |     0.0665     |\n",
      "| mean precision_at_recall_10 |  0.0580  | 0.0764 |    0.0412    |     0.0323     |   0.0499  |     0.0665     |\n",
      "|    mean average_precision   |  0.1116  | 0.1524 |    0.0842    |     0.0546     |   0.0980  |     0.1033     |\n",
      "|        mean f_measure       |  0.0054  | 0.0053 |    0.0049    |     0.0026     |   0.0038  |     0.0038     |\n",
      "|         mean recall         |  0.7404  | 0.7418 |    0.6818    |     0.4203     |   0.6447  |     0.5820     |\n",
      "+-----------------------------+----------+--------+--------------+----------------+-----------+----------------+\n",
      "\n",
      "\u001B[1;34mThe best model based on mean average precision (MAP) is: dfr\u001B[0m\n",
      "+-----------------------------+----------+--------+---------------+-----------------+------------+-----------------+\n",
      "|            Metric           | baseline |  bm25  | bm25_standard | bm25_whitespace | bm25_ngram | bm25_edge_ngram |\n",
      "+-----------------------------+----------+--------+---------------+-----------------+------------+-----------------+\n",
      "|      mean success_at_1      |  0.1053  | 0.2105 |     0.1053    |      0.0526     |   0.0789   |      0.0789     |\n",
      "|      mean success_at_5      |  0.2632  | 0.2895 |     0.2105    |      0.1053     |   0.1579   |      0.2105     |\n",
      "|      mean success_at_10     |  0.3158  | 0.3684 |     0.2895    |      0.1053     |   0.2368   |      0.2895     |\n",
      "|       mean r_precision      |  0.0916  | 0.1147 |     0.0467    |      0.0338     |   0.0463   |      0.0360     |\n",
      "|     mean precision_at_1     |  0.1053  | 0.2105 |     0.1053    |      0.0526     |   0.0789   |      0.0789     |\n",
      "|     mean precision_at_5     |  0.0789  | 0.0842 |     0.0632    |      0.0368     |   0.0421   |      0.0632     |\n",
      "|     mean precision_at_10    |  0.0474  | 0.0605 |     0.0447    |      0.0211     |   0.0316   |      0.0500     |\n",
      "|     mean precision_at_50    |  0.0195  | 0.0242 |     0.0200    |      0.0074     |   0.0158   |      0.0195     |\n",
      "|    mean precision_at_100    |  0.0139  | 0.0158 |     0.0129    |      0.0045     |   0.0103   |      0.0113     |\n",
      "| mean precision_at_recall_00 |  0.2015  | 0.2606 |     0.1529    |      0.0818     |   0.1283   |      0.1494     |\n",
      "| mean precision_at_recall_01 |  0.1898  | 0.2463 |     0.1397    |      0.0816     |   0.1247   |      0.1363     |\n",
      "| mean precision_at_recall_02 |  0.1683  | 0.2203 |     0.1348    |      0.0809     |   0.0985   |      0.1006     |\n",
      "| mean precision_at_recall_03 |  0.1333  | 0.1852 |     0.1077    |      0.0544     |   0.0969   |      0.0943     |\n",
      "| mean precision_at_recall_04 |  0.1236  | 0.1697 |     0.0856    |      0.0461     |   0.0882   |      0.0833     |\n",
      "| mean precision_at_recall_05 |  0.1227  | 0.1632 |     0.0840    |      0.0458     |   0.0867   |      0.0747     |\n",
      "| mean precision_at_recall_06 |  0.0874  | 0.1005 |     0.0472    |      0.0282     |   0.0500   |      0.0555     |\n",
      "| mean precision_at_recall_07 |  0.0843  | 0.0967 |     0.0435    |      0.0275     |   0.0469   |      0.0529     |\n",
      "| mean precision_at_recall_08 |  0.0600  | 0.0668 |     0.0334    |      0.0182     |   0.0414   |      0.0519     |\n",
      "| mean precision_at_recall_09 |  0.0580  | 0.0656 |     0.0322    |      0.0182     |   0.0412   |      0.0511     |\n",
      "| mean precision_at_recall_10 |  0.0580  | 0.0656 |     0.0322    |      0.0182     |   0.0412   |      0.0511     |\n",
      "|    mean average_precision   |  0.1116  | 0.1441 |     0.0778    |      0.0448     |   0.0728   |      0.0774     |\n",
      "|        mean f_measure       |  0.0054  | 0.0053 |     0.0050    |      0.0025     |   0.0039   |      0.0042     |\n",
      "|         mean recall         |  0.7404  | 0.7512 |     0.6920    |      0.4261     |   0.6568   |      0.6079     |\n",
      "+-----------------------------+----------+--------+---------------+-----------------+------------+-----------------+\n",
      "\n",
      "\u001B[1;34mThe best model based on mean average precision (MAP) is: bm25\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "dfr_models = [\"baseline\", \"dfr\", \"dfr_standard\", \"dfr_whitespace\", \"dfr_ngram\", \"dfr_edge_ngram\"]\n",
    "dfr_results = evaluate_models(dfr_models)\n",
    "print_results_table(dfr_results)\n",
    "\n",
    "bm25_models = [\"baseline\", \"bm25\", \"bm25_standard\", \"bm25_whitespace\", \"bm25_ngram\", \"bm25_edge_ngram\"]\n",
    "bm25_results = evaluate_models(bm25_models)\n",
    "print_results_table(bm25_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T18:15:24.018268525Z",
     "start_time": "2023-11-10T18:13:41.166711034Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
