{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T14:15:20.209567092Z",
     "start_time": "2023-11-10T14:15:20.120938505Z"
    }
   },
   "outputs": [],
   "source": [
    "import elasticsearch.helpers\n",
    "import json\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def success_at_1 (relevant, retrieved):\n",
    "    if len(retrieved) > 0 and retrieved[0] in relevant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def success_at_5(relevant, retrieved):\n",
    "    for retrieved_single in retrieved[:5]:\n",
    "        if retrieved_single in relevant:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def success_at_10(relevant, retrieved):\n",
    "    for retrieved_single in retrieved[:10]:\n",
    "        if retrieved_single in relevant:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def precision(relevant, retrieved):\n",
    "    relevant_items_retrieved_counter = 0\n",
    "    for r in retrieved:\n",
    "        if r in relevant:\n",
    "            relevant_items_retrieved_counter += 1\n",
    "    return relevant_items_retrieved_counter / len(retrieved) if len(retrieved) > 0 else 0.0\n",
    "    \n",
    "def recall(relevant, retrieved):\n",
    "    relevant_items_retrieved_counter = 0\n",
    "    for r in retrieved:\n",
    "        if r in relevant:\n",
    "            relevant_items_retrieved_counter += 1\n",
    "    return relevant_items_retrieved_counter / len(relevant)\n",
    "    \n",
    "def f_measure(relevant, retrieved):\n",
    "    P = precision(relevant, retrieved)\n",
    "    R = recall(relevant, retrieved)\n",
    "    if P + R == 0.0:\n",
    "        return 0.0\n",
    "    return 2 * P * R / (P + R)\n",
    "    \n",
    "def precision_at_k(relevant, retrieved, k):\n",
    "    return precision(relevant, retrieved[:k])\n",
    "\n",
    "def r_precision(relevant, retrieved):\n",
    "    return precision(relevant, retrieved[:len(relevant)])\n",
    "\n",
    "def interpolated_precision_at_recall_X (relevant, retrieved, X):\n",
    "    max_precision = 0.0\n",
    "    for i in range(len(retrieved)):\n",
    "        if recall(relevant, retrieved[:i]) >= X:\n",
    "            precision_at_i = precision_at_k(relevant, retrieved, i)\n",
    "            if precision_at_i > max_precision:\n",
    "                max_precision = precision_at_i\n",
    "    return max_precision\n",
    "\n",
    "def average_precision(relevant, retrieved):\n",
    "    sum_avg = 0.0\n",
    "    for retrieved_index in range(len(retrieved)):\n",
    "        if retrieved[retrieved_index] in relevant:\n",
    "            sum_avg += precision_at_k(relevant, retrieved, retrieved_index + 1)\n",
    "    return sum_avg / len(relevant)\n",
    "    \n",
    "def read_qrels_file(qrels_file):  # reads the content of he qrels file\n",
    "    trec_relevant = dict()  # query_id -> set([docid1, docid2, ...])\n",
    "    with open(qrels_file, 'r') as qrels:\n",
    "        for line in qrels:\n",
    "            (qid, q0, doc_id, rel) = line.strip().split()\n",
    "            if qid not in trec_relevant:\n",
    "                trec_relevant[qid] = set()\n",
    "            if rel == \"1\":\n",
    "                trec_relevant[qid].add(doc_id)\n",
    "    return trec_relevant\n",
    "\n",
    "def read_run_file(run_file):  \n",
    "    # read the content of the run file produced by our IR system \n",
    "    # (in the following exercises you will create your own run_files)\n",
    "    trec_retrieved = dict()  # query_id -> [docid1, docid2, ...]\n",
    "    with open(run_file, 'r') as run:\n",
    "        for line in run:\n",
    "            (qid, q0, doc_id, rank, score, tag) = line.strip().split()\n",
    "            if qid not in trec_retrieved:\n",
    "                trec_retrieved[qid] = []\n",
    "            trec_retrieved[qid].append(doc_id) \n",
    "    return trec_retrieved\n",
    "    \n",
    "def read_eval_files(qrels_file, run_file):\n",
    "    return read_qrels_file(qrels_file), read_run_file(run_file)\n",
    "\n",
    "def mean_average_precision(all_relevant, all_retrieved):    \n",
    "    sum_map = []\n",
    "    for key in all_retrieved:\n",
    "        sum_map.append(average_precision(all_relevant[key], all_retrieved[key]))\n",
    "    return sum(sum_map) / len(sum_map)\n",
    "\n",
    "def mean_metric(measure, all_relevant, all_retrieved):\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for qid in all_relevant:\n",
    "        relevant  = all_relevant[qid]\n",
    "        retrieved = all_retrieved.get(qid, [])\n",
    "        value = measure(relevant, retrieved)\n",
    "        total += value\n",
    "        count += 1\n",
    "    return \"mean \" + measure.__name__, total / count\n",
    "\n",
    "def trec_eval(qrels_file, run_file):\n",
    "    def precision_at_1(rel, ret): return precision_at_k(rel, ret, k=1)\n",
    "    def precision_at_5(rel, ret): return precision_at_k(rel, ret, k=5)\n",
    "    def precision_at_10(rel, ret): return precision_at_k(rel, ret, k=10)\n",
    "    def precision_at_50(rel, ret): return precision_at_k(rel, ret, k=50)\n",
    "    def precision_at_100(rel, ret): return precision_at_k(rel, ret, k=100)\n",
    "    def precision_at_recall_00(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.0)\n",
    "    def precision_at_recall_01(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.1)\n",
    "    def precision_at_recall_02(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.2)\n",
    "    def precision_at_recall_03(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.3)\n",
    "    def precision_at_recall_04(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.4)\n",
    "    def precision_at_recall_05(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.5)\n",
    "    def precision_at_recall_06(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.6)\n",
    "    def precision_at_recall_07(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.7)\n",
    "    def precision_at_recall_08(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.8)\n",
    "    def precision_at_recall_09(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=0.9)\n",
    "    def precision_at_recall_10(rel, ret): return interpolated_precision_at_recall_X(rel, ret, X=1.0)\n",
    "\n",
    "    (all_relevant, all_retrieved) = read_eval_files(qrels_file, run_file)\n",
    "    \n",
    "    unknown_qids = set(all_retrieved.keys()).difference(all_relevant.keys())\n",
    "    if len(unknown_qids) > 0:\n",
    "        raise ValueError(\"Unknown qids in run: {}\".format(sorted(list(unknown_qids))))\n",
    "\n",
    "    metrics = [success_at_1,\n",
    "               success_at_5,\n",
    "               success_at_10,\n",
    "               r_precision,\n",
    "               precision_at_1,\n",
    "               precision_at_5,\n",
    "               precision_at_10,\n",
    "               precision_at_50,\n",
    "               precision_at_100,\n",
    "               precision_at_recall_00,\n",
    "               precision_at_recall_01,\n",
    "               precision_at_recall_02,\n",
    "               precision_at_recall_03,\n",
    "               precision_at_recall_04,\n",
    "               precision_at_recall_05,\n",
    "               precision_at_recall_06,\n",
    "               precision_at_recall_07,\n",
    "               precision_at_recall_08,\n",
    "               precision_at_recall_09,\n",
    "               precision_at_recall_10,\n",
    "               average_precision,\n",
    "               f_measure,\n",
    "               recall]\n",
    "    return [mean_metric(metric, all_relevant, all_retrieved) for metric in metrics]\n",
    "\n",
    "def print_trec_eval(qrels_file, run_file):\n",
    "    results_var = trec_eval(qrels_file, run_file)\n",
    "    print(\"Results for {}\".format(run_file))\n",
    "    for (metric, score) in results_var:\n",
    "        print(\"{:<30} {:.4}\".format(metric, score))\n",
    "        \n",
    "def sign_test_values(measure, qrels_file, run_file_1, run_file_2):\n",
    "    all_relevant = read_qrels_file(qrels_file)\n",
    "    all_retrieved_1 = read_run_file(run_file_1)\n",
    "    all_retrieved_2 = read_run_file(run_file_2)\n",
    "    better = 0\n",
    "    worse  = 0\n",
    "    \n",
    "    for key in all_relevant:\n",
    "        measure_file_1 = measure(all_relevant[key], all_retrieved_1[key])\n",
    "        measure_file_2 = measure(all_relevant[key], all_retrieved_2[key])\n",
    "        if measure_file_1 > measure_file_2:\n",
    "            better += 1\n",
    "        elif measure_file_1 <  measure_file_2:\n",
    "            worse += 1\n",
    "    \n",
    "    return better, worse\n",
    "    \n",
    "def precision_at_rank_5(rel, ret):\n",
    "    return precision_at_k(rel, ret, k=5)\n",
    "\n",
    "\n",
    "def read_documents(file_name):\n",
    "    \"\"\"\n",
    "    Returns a generator of documents to be indexed by elastic, read from file_name\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as documents:\n",
    "        for line in documents:\n",
    "            doc_line = json.loads(line)\n",
    "            if 'index' in doc_line:\n",
    "                id_param = doc_line['index']['_id']\n",
    "            elif 'PMID' in doc_line:\n",
    "                doc_line['_id'] = id_param\n",
    "                yield doc_line\n",
    "            else:\n",
    "                raise ValueError('Woops, error in index file')\n",
    "\n",
    "def create_index(es_param, index_name, body=None):\n",
    "    # delete index when it already exists\n",
    "    if body is None:\n",
    "        body = {}\n",
    "    es_param.indices.delete(index=index_name, ignore=[400, 404])\n",
    "    # create the index \n",
    "    es_param.indices.create(index=index_name, body=body)\n",
    "                \n",
    "def index_documents(es_param, collection_file_name, index_name, body=None):\n",
    "    if body is None:\n",
    "        body = {}\n",
    "    create_index(es_param, index_name, body)\n",
    "    # bulk index the documents from file_name\n",
    "    return elasticsearch.helpers.bulk(\n",
    "        es_param, \n",
    "        read_documents(collection_file_name),\n",
    "        index=index_name,\n",
    "        chunk_size=2000,\n",
    "        request_timeout=30\n",
    "    )\n",
    "\n",
    "def make_trec_run(es_param, topics_file_name, run_file_name, index_name=\"genomics\", run_name=\"test\"):\n",
    "    with open(f\"run/{run_file_name}\", 'w') as run_file:\n",
    "        with open(topics_file_name, 'r') as test_queries:\n",
    "            for line in test_queries:\n",
    "                (qid, query) = line.strip().split('\\t')\n",
    "                \n",
    "                search_query = {\n",
    "                    \"query\": {\n",
    "                        \"bool\": {\n",
    "                            \"should\": [\n",
    "                                {\"match\": {\"TI\": query}},  # Search in the title field.\n",
    "                                {\"match\": {\"AB\": query}}   # Search in the abstract field.\n",
    "                            ]\n",
    "                        }\n",
    "                    },\n",
    "                    \"size\": 1000  # You can adjust the number of results as needed.\n",
    "                }\n",
    "                \n",
    "                # Execute the query against the specified Elasticsearch index.\n",
    "                search_results = es_param.search(index=index_name, body=search_query)\n",
    "                \n",
    "                # Process the search results and write them to the run file in TREC format.\n",
    "                for rank, hit in enumerate(search_results['hits']['hits']):\n",
    "                    pmid = hit['_source']['PMID']\n",
    "                    score = hit['_score']\n",
    "                    run_line = f\"{qid} Q0 {pmid} {rank + 1} {score} {run_name}\\n\"\n",
    "                    run_file.write(run_line)\n",
    "                # \n",
    "\n",
    "def evaluate_models(models_param, qrels_file):\n",
    "    results_var = {}\n",
    "    for model_name in models_param:\n",
    "        run_file_name = f\"run/{model_name}.run\"\n",
    "        eval_results = trec_eval(qrels_file, run_file_name)\n",
    "        results_var[model_name] = {metric: score for metric, score in eval_results}\n",
    "    return results_var\n",
    "\n",
    "def determine_best_model(results_param, metric):\n",
    "    \"\"\"Determine the best model based on a specific metric.\"\"\"\n",
    "    return max(results_param, key=lambda model: results_param[model][metric])\n",
    "\n",
    "def print_results_table(results_param):\n",
    "    # Extract model names\n",
    "    model_names = list(results_param.keys())\n",
    "    \n",
    "    # Extract metric names from the first model's results\n",
    "    metrics = list(next(iter(results_param.values())).keys())\n",
    "    \n",
    "    # Create a table with headers\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metric\"] + model_names\n",
    "    \n",
    "    # Add rows for each metric's results across all models\n",
    "    for metric in metrics:\n",
    "        values = [metric] + [f\"{results_param[model_name][metric]:.4f}\" for model_name in model_names]\n",
    "        table.add_row(values)\n",
    "    \n",
    "    # Print the table\n",
    "    print(table)\n",
    "\n",
    "    # Determine and print the best model based on mean average precision (MAP)\n",
    "    best_model = determine_best_model(results_param, 'mean average_precision')\n",
    "    print(f\"\\n\\033[1;34mThe best model based on mean average precision (MAP) is: {best_model}\\033[0m\")  # Emphasize with blue color\n",
    "    \n",
    "def generate_dfr_tokenization(strategy):\n",
    "    return {\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"similarity\": {\n",
    "                    \"custom_dfr_settings\": {\n",
    "                        \"type\": \"DFR\",\n",
    "                        \"basic_model\": \"g\",\n",
    "                        \"after_effect\": \"l\",\n",
    "                        \"normalization\": \"h2\",\n",
    "                        \"normalization.h2.c\": \"3.0\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"tokenizer\": {\n",
    "                    \"my_tokenizer\": strategy\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"my_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"my_tokenizer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"TI\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"similarity\": \"custom_dfr_settings\",\n",
    "                    \"analyzer\": \"my_analyzer\"\n",
    "                },\n",
    "                \"AB\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"similarity\": \"custom_dfr_settings\",\n",
    "                    \"analyzer\": \"my_analyzer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "es = elasticsearch.Elasticsearch('http://localhost:9200')\n",
    "\n",
    "def dfr_index(body_dfr):\n",
    "    body_type_v = body_dfr[\"type\"]\n",
    "    generated_body = generate_dfr_tokenization(body_dfr)\n",
    "    index_documents(es, 'data01/FIR-s05-medline.json', f\"genomics-{body_type_v}\", body=generated_body)\n",
    "    return body_type_v\n",
    "\n",
    "def dfr_run(body_type_file):\n",
    "    make_trec_run(es, 'data01/FIR-s05-training-queries-simple.txt', f\"dfr_{body_type_file}.run\", f\"genomics-{body_type_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = dfr_index({\n",
    "    \"type\": \"standard\"\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfr_run(body_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = dfr_index({\n",
    "    \"type\": \"whitespace\"\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfr_run(body_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = dfr_index({\n",
    "    \"type\": \"keyword\",\n",
    "    \"token_chars\": [\n",
    "        \"letter\",\n",
    "        \"digit\"\n",
    "    ]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfr_run(body_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = dfr_index({\n",
    "    \"type\": \"ngram\",\n",
    "    \"min_gram\": 4,\n",
    "    \"max_gram\": 5,\n",
    "    \"token_chars\": [\n",
    "        \"letter\",\n",
    "        \"digit\"\n",
    "    ]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfr_run(body_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "body_type = dfr_index({\n",
    "    \"type\": \"edge_ngram\",\n",
    "    \"min_gram\": 2,\n",
    "    \"max_gram\": 20,\n",
    "    \"token_chars\": [\n",
    "        \"letter\",\n",
    "        \"digit\"\n",
    "    ]\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfr_run(body_type)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+--------------+----------------+-------------+----------------+-----------+\n",
      "|            Metric           | dfr_standard | dfr_whitespace | dfr_keyword | dfr_edge_ngram | dfr_ngram |\n",
      "+-----------------------------+--------------+----------------+-------------+----------------+-----------+\n",
      "|      mean success_at_1      |    0.1579    |     0.0526     |    0.0000   |     0.1053     |   0.1053  |\n",
      "|      mean success_at_5      |    0.2895    |     0.1053     |    0.0000   |     0.2368     |   0.2368  |\n",
      "|      mean success_at_10     |    0.4211    |     0.1316     |    0.0000   |     0.2895     |   0.2895  |\n",
      "|       mean r_precision      |    0.1188    |     0.0566     |    0.0000   |     0.0870     |   0.0730  |\n",
      "|     mean precision_at_1     |    0.1579    |     0.0526     |    0.0000   |     0.1053     |   0.1053  |\n",
      "|     mean precision_at_5     |    0.0947    |     0.0368     |    0.0000   |     0.0632     |   0.0632  |\n",
      "|     mean precision_at_10    |    0.0658    |     0.0211     |    0.0000   |     0.0421     |   0.0421  |\n",
      "|     mean precision_at_50    |    0.0253    |     0.0079     |    0.0000   |     0.0189     |   0.0163  |\n",
      "|    mean precision_at_100    |    0.0158    |     0.0047     |    0.0000   |     0.0113     |   0.0111  |\n",
      "| mean precision_at_recall_00 |    0.2387    |     0.0928     |    0.0000   |     0.1761     |   0.1832  |\n",
      "| mean precision_at_recall_01 |    0.2387    |     0.0921     |    0.0000   |     0.1735     |   0.1783  |\n",
      "| mean precision_at_recall_02 |    0.2289    |     0.0907     |    0.0000   |     0.1466     |   0.1526  |\n",
      "| mean precision_at_recall_03 |    0.2001    |     0.0640     |    0.0000   |     0.1352     |   0.1498  |\n",
      "| mean precision_at_recall_04 |    0.1795    |     0.0513     |    0.0000   |     0.1113     |   0.1290  |\n",
      "| mean precision_at_recall_05 |    0.1732    |     0.0512     |    0.0000   |     0.1050     |   0.1170  |\n",
      "| mean precision_at_recall_06 |    0.1148    |     0.0407     |    0.0000   |     0.0775     |   0.0557  |\n",
      "| mean precision_at_recall_07 |    0.1109    |     0.0399     |    0.0000   |     0.0704     |   0.0537  |\n",
      "| mean precision_at_recall_08 |    0.0772    |     0.0323     |    0.0000   |     0.0676     |   0.0503  |\n",
      "| mean precision_at_recall_09 |    0.0764    |     0.0323     |    0.0000   |     0.0665     |   0.0499  |\n",
      "| mean precision_at_recall_10 |    0.0764    |     0.0323     |    0.0000   |     0.0665     |   0.0499  |\n",
      "|    mean average_precision   |    0.1524    |     0.0546     |    0.0000   |     0.1033     |   0.0980  |\n",
      "|        mean f_measure       |    0.0053    |     0.0026     |    0.0000   |     0.0038     |   0.0038  |\n",
      "|         mean recall         |    0.7418    |     0.4203     |    0.0000   |     0.5820     |   0.6447  |\n",
      "+-----------------------------+--------------+----------------+-------------+----------------+-----------+\n",
      "\n",
      "\u001B[1;34mThe best model based on mean average precision (MAP) is: dfr_standard\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# models = [\"boolean\", \"dirichelet\", \"lmjelinekmercer\", \"baseline\", \"dfr_settings\", \"pattern_settings\"]\n",
    "models = [\"dfr_standard\", \"dfr_whitespace\", \"dfr_keyword\", \"dfr_edge_ngram\", \"dfr_ngram\"]\n",
    "results = evaluate_models(models, 'data01/FIR-s05-training-qrels.txt')\n",
    "print_results_table(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T14:49:14.488058042Z",
     "start_time": "2023-11-10T14:48:34.371092190Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
